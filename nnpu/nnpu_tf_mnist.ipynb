{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import abc\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "UNL = 0\n",
    "POS = 1\n",
    "NEG = -1\n",
    "\n",
    "DF = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipes\n",
    "- Should be replaced with TF dataset APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIterator(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_lists,\n",
    "        batch_size,\n",
    "        max_epoch=None,\n",
    "        repeat=True,\n",
    "        shuffle=True,\n",
    "        epoch_finished=None,\n",
    "    ):\n",
    "        for idx in range(len(data_lists) - 1):\n",
    "            assert len(data_lists[idx]) == len(data_lists[idx + 1])\n",
    "        self._data = data_lists\n",
    "        self._batch_size = batch_size\n",
    "        self._repeat = repeat\n",
    "        self._shuffle = shuffle\n",
    "        self._num_data = len(self._data[0])\n",
    "        assert self._num_data >= self._batch_size\n",
    "        self._shuffle_indexes = self._maybe_generate_shuffled_indexes()\n",
    "        self._epoch_finished = 0 if epoch_finished is None else epoch_finished\n",
    "        self._max_epoch = max_epoch\n",
    "\n",
    "    @property\n",
    "    def num_data(self):\n",
    "        return self._num_data\n",
    "\n",
    "    @property\n",
    "    def finished(self):\n",
    "        if not self._repeat:\n",
    "            if self.epoch_finished == 1:\n",
    "                return True\n",
    "        if self._max_epoch is not None:\n",
    "            return self.epoch_finished > self._max_epoch\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    @property\n",
    "    def epoch_finished(self):\n",
    "        return self._epoch_finished\n",
    "\n",
    "    def _maybe_generate_shuffled_indexes(self):\n",
    "        indexes = list(range(self._num_data))\n",
    "        if self._shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "        return indexes\n",
    "\n",
    "    def get_next_batch(self, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self._batch_size\n",
    "        else:\n",
    "            assert self._num_data >= batch_size\n",
    "        if len(self._shuffle_indexes) == 0:\n",
    "            raise StopIteration()\n",
    "        if len(self._shuffle_indexes) >= batch_size:  # when data left is enough\n",
    "            indexes = self._shuffle_indexes[:batch_size]\n",
    "            self._shuffle_indexes = self._shuffle_indexes[batch_size:]\n",
    "        else:  # when data left is not enough.\n",
    "            indexes = self._shuffle_indexes\n",
    "            self._shuffle_indexes = []\n",
    "        if len(self._shuffle_indexes) == 0:\n",
    "            self._epoch_finished += 1\n",
    "            if self._repeat:\n",
    "                if self._max_epoch is not None:\n",
    "                    if self._epoch_finished > self._max_epoch:\n",
    "                        raise StopIteration()\n",
    "                self._shuffle_indexes = self._maybe_generate_shuffled_indexes()\n",
    "                num_left = batch_size - len(indexes)\n",
    "                indexes.extend(self._shuffle_indexes[:num_left])\n",
    "                self._shuffle_indexes = self._shuffle_indexes[num_left:]\n",
    "        return [l[indexes] for l in self._data]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.get_next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PuDataIterator(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        u_data,\n",
    "        l_data,\n",
    "        batch_size,\n",
    "        max_epoch=None,\n",
    "        epoch_finished=0,\n",
    "        repeat=True,\n",
    "        shuffle=True,\n",
    "    ):\n",
    "        self._u_num = u_data[0].shape[0]\n",
    "        self._l_num = l_data[0].shape[0]\n",
    "        self._data_num = self._u_num + self._l_num\n",
    "        self._p_u = float(self._u_num) / float(self._u_num + self._l_num)\n",
    "        self._p_l = float(self._l_num) / float(self._u_num + self._l_num)\n",
    "        self._batch_size = batch_size\n",
    "        self._used_u_num, self._used_l_num = 0, 0\n",
    "        self._u_iterator = DataIterator(\n",
    "            u_data,\n",
    "            int(batch_size * self._p_u),\n",
    "            repeat=repeat,\n",
    "            shuffle=shuffle,\n",
    "            epoch_finished=epoch_finished,\n",
    "            max_epoch=max_epoch,\n",
    "        )\n",
    "        self._l_iterator = DataIterator(\n",
    "            l_data,\n",
    "            int(batch_size * self._p_l),\n",
    "            repeat=repeat,\n",
    "            shuffle=shuffle,\n",
    "            epoch_finished=epoch_finished,\n",
    "            max_epoch=max_epoch,\n",
    "        )\n",
    "        self._finished_epoch = epoch_finished\n",
    "        self._max_epoch = max_epoch\n",
    "        self._repeat = repeat\n",
    "        self._shuffle = shuffle\n",
    "\n",
    "    @property\n",
    "    def epoch_finished(self):\n",
    "        return self._finished_epoch\n",
    "\n",
    "    @property\n",
    "    def num_data(self):\n",
    "        return self._data_num\n",
    "\n",
    "    @property\n",
    "    def finished(self):\n",
    "        if self._max_epoch is not None:\n",
    "            return self.epoch_finished > self._max_epoch\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __next__(self):\n",
    "        used_num = self._used_l_num + self._used_u_num + self._batch_size\n",
    "        next_u_num = round(used_num * self._p_u - self._used_u_num)\n",
    "        self._used_u_num += next_u_num\n",
    "        next_l_num = round(used_num * self._p_l - self._used_l_num)\n",
    "        next_l_num += self._batch_size - next_u_num - next_l_num\n",
    "        self._used_l_num += next_l_num\n",
    "        # Whatever the case, at least one sample is expected from each iterator\n",
    "        # (though the iterator may be empty, when self._repeat == False).\n",
    "        assert next_l_num != 0 and next_u_num != 0\n",
    "\n",
    "        if self._max_epoch is not None:\n",
    "            if self._finished_epoch >= self._max_epoch:\n",
    "                raise StopIteration()\n",
    "        # Stop iteration only if both iterator is finished. So no data will\n",
    "        # be missed.\n",
    "        if self._u_iterator.finished and self._l_iterator.finished:\n",
    "            raise StopIteration()\n",
    "\n",
    "        try:\n",
    "            u_data = self._u_iterator.get_next_batch(int(next_u_num))\n",
    "        except StopIteration:\n",
    "            u_data = None\n",
    "\n",
    "        try:\n",
    "            l_data = self._l_iterator.get_next_batch(int(next_l_num))\n",
    "        except StopIteration:\n",
    "            l_data = None\n",
    "\n",
    "        if not self._repeat:\n",
    "\n",
    "            # It is guaranteed here that, if one of the iterator is finished,\n",
    "            # another one will make up the missing part.\n",
    "            if self._u_iterator.finished and not self._l_iterator.finished:\n",
    "                u_num = 0 if u_data is None else u_data[0].shape[0]\n",
    "                left = self._l_iterator.get_next_batch(int(next_u_num - u_num))\n",
    "                l_data = [\n",
    "                    np.concatenate((l_data[i], left[i])) for i in range(len(l_data))\n",
    "                ]\n",
    "            if self._l_iterator.finished and not self._u_iterator.finished:\n",
    "                l_num = 0 if l_data is None else l_data[0].shape[0]\n",
    "                left = self._u_iterator.get_next_batch(int(next_l_num - l_num))\n",
    "                u_data = [\n",
    "                    np.concatenate((u_data[i], left[i])) for i in range(len(u_data))\n",
    "                ]\n",
    "\n",
    "        self._finished_epoch = min(\n",
    "            self._u_iterator.epoch_finished, self._l_iterator.epoch_finished\n",
    "        )\n",
    "        if u_data is None:\n",
    "            return l_data\n",
    "        elif l_data is None:\n",
    "            return u_data\n",
    "        else:\n",
    "            return [np.concatenate((u_data[i], l_data[i])) for i in range(len(l_data))]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PuLearningDataSet(object):\n",
    "    def __init__(self, cfg):\n",
    "        \n",
    "        self.max_epoch = cfg[\"max_epoch\"]\n",
    "        self.batch_size = cfg[\"batch_size\"]\n",
    "        self.prior = cfg[\"prior\"]\n",
    "        \n",
    "        (_x, _y), _ = DF.load_data()\n",
    "        _x= _x / 255.0\n",
    "        _x = _x[..., tf.newaxis].astype(np.float32)\n",
    "        _y = _y.astype(np.int32)\n",
    "        \n",
    "        _y[_y % 2 == 1] = UNL\n",
    "        _y[(_y % 2 == 0) & (_y != UNL)] = POS\n",
    "        \n",
    "        # This is ugly, but edit here to adjust how you want to specify the number of positive samples\n",
    "        _y[(_y == POS)][:int(60000*self.prior)] = UNL\n",
    "        \n",
    "        self.num_labeled = (_y == POS).sum()\n",
    "        \n",
    "        shuffled_indexes = np.array(range(len(_y)))\n",
    "        np.random.shuffle(shuffled_indexes)\n",
    "        _y = _y[shuffled_indexes]\n",
    "        _x = _x[shuffled_indexes]\n",
    "        \n",
    "        unlabeled_mask = (_y == UNL)\n",
    "        labeled_mask = (_y == POS)\n",
    "        \n",
    "        _y = np.expand_dims(_y, axis=1)\n",
    "        \n",
    "        u_x = _x[unlabeled_mask]\n",
    "        u_y = _y[unlabeled_mask]\n",
    "        \n",
    "        l_x = _x[labeled_mask]\n",
    "        l_y = _y[labeled_mask]\n",
    "        \n",
    "        self.iterator = PuDataIterator(\n",
    "            (u_x, u_y),\n",
    "            (l_x, l_y),\n",
    "            self.batch_size,\n",
    "            max_epoch=self.max_epoch,\n",
    "            repeat=True,\n",
    "            shuffle=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Def."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_losses(network_out, labels, prior):\n",
    "    assert network_out.shape.ndims == 2\n",
    "    assert network_out.shape[1] == 1\n",
    "    loss_func = lambda network_out, y: tf.nn.sigmoid(\n",
    "        -network_out * y\n",
    "    )\n",
    "    positive = tf.cast(tf.equal(labels, POS), tf.float32)\n",
    "    unlabeled = tf.cast(tf.equal(labels, UNL), tf.float32)\n",
    "    num_positive = tf.maximum(1.0, tf.reduce_sum(positive))\n",
    "    num_unlabeled = tf.maximum(1.0, tf.reduce_sum(unlabeled))\n",
    "    losses_positive = loss_func(network_out, POS)\n",
    "    losses_negative = loss_func(network_out, NEG)\n",
    "    positive_risk = tf.reduce_sum(\n",
    "        prior * positive / num_positive * losses_positive\n",
    "    )\n",
    "    negative_risk = tf.reduce_sum(\n",
    "        (unlabeled / num_unlabeled - prior * positive / num_positive) * losses_negative,\n",
    "    )\n",
    "    return positive_risk, negative_risk\n",
    "\n",
    "def nnpu_loss(network_out, labels):\n",
    "    positive_risk, negative_risk = calculate_losses(\n",
    "        network_out, labels, pu_dataset.prior\n",
    "    )\n",
    "    is_ga = tf.less(negative_risk, -0.0)\n",
    "    return tf.cond(\n",
    "        is_ga,\n",
    "        lambda: -1.0 * negative_risk,\n",
    "        lambda: positive_risk + negative_risk,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Def."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A custom training loop\n",
    "Note: only that it works, not optimized for testing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "gs_model = MNISTModel()\n",
    "\n",
    "@tf.function\n",
    "def train_step(data, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = gs_model(data, training=True)\n",
    "        loss = nnpu_loss(predictions, labels)\n",
    "\n",
    "    gradients = tape.gradient(loss, gs_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, gs_model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "\n",
    "cfg = {\n",
    "    \"batch_size\": 128,\n",
    "    \"max_epoch\": 1,\n",
    "    \"prior\": 0.3\n",
    "}\n",
    "print(cfg)\n",
    "\n",
    "EPOCHS = 10\n",
    "step = 0\n",
    "\n",
    "dt_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"./log/{}\".format(dt_str)\n",
    "print('log_dir={}'.format(log_dir))\n",
    "\n",
    "model_dir = \"./model/{}\".format(dt_str)\n",
    "print('model_dir={}'.format(model_dir))\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_summary_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"train\"))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    pu_dataset = PuLearningDataSet(cfg)\n",
    "\n",
    "    for data, labels in pu_dataset.iterator:\n",
    "        step += 1\n",
    "        train_step(data, labels)\n",
    "        # print(train_loss.result().numpy(), end='>', flush=True)\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=step)\n",
    "            \n",
    "    print(epoch, \", loss=\", train_loss.result().numpy(), flush=True)\n",
    "\n",
    "    gs_model.save(\n",
    "        filepath=model_dir,\n",
    "        overwrite=True,\n",
    "        include_optimizer=True,\n",
    "        save_format=\"tf\"\n",
    "    )\n",
    "    # print('Model saved to {}'.format(model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.731093 ],\n",
       "       [-4.939483 ],\n",
       "       [ 4.5985384],\n",
       "       ...,\n",
       "       [-3.303848 ],\n",
       "       [ 4.30934  ],\n",
       "       [ 3.5497808]], dtype=float32)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(_x, _y), _ = DF.load_data()\n",
    "_x= _x / 255.0\n",
    "_x = _x[..., tf.newaxis].astype(np.float32)\n",
    "_y = _y.astype(np.int32)\n",
    "\n",
    "loaded_model = tf.saved_model.load(model_dir)\n",
    "f = loaded_model.signatures[\"serving_default\"]\n",
    "pred = f(tf.constant(_x, dtype=tf.float32))['output_1'].numpy()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6484833333333333"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred>0).squeeze() == (_y>0)).sum() / len(_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
